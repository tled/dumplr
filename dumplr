#!/bin/env python3
#-*- coding: utf-8 -*-

"""
                |                /     
             ___|      _ _  ___ (  ___ 
            |   )|   )| | )|   )| |   )
            |__/ |__/ |  / |__/ | |    
                           |           
          dumps your favorite tumblr blog
"""

import Scrapelr.scrapelr as Dumplr
import argparse

if __name__ == '__main__':
    print(__doc__)
    argp = argparse.ArgumentParser(description='Downloads images from a tumblr blog')
    argp.add_argument('-a', '--all', action='store_true', help="All blog pages")
    argp.add_argument('-l', '--brute-force-limit', type=int, default = 3, help="Tries next X pages, if one has no valid images")
    argp.add_argument('--destination','-d', dest='path', default='.', help="Destination directory.")
    argp.add_argument('BlogURL', help="Blog URL, e.g. “foo.tumblr.com”.")
    argp.add_argument('Pages', metavar="N", nargs='*', help="Page(s) to dump. Use “N-M” to specify a range.")
    args = argp.parse_args()

    targetpath = args.path
    blog       = args.BlogURL
    allpages   = args.all
    bflimit    = args.brute_force_limit
    pages      = []

    for p in args.Pages:
        if p == '-':
            raise ValueError
        p = p.split('-')
        if len(p) > 1:
            [ pages.append(i) for i in range(int(p[0]), int(p[1])+1 )]
        else:
            pages.append(int(p[0]))

    #setting verbose and debug output
    Dumplr.VERBOSE = True
    Dumplr.DEBUG   = False

    if not allpages and len(pages) == 0:
        print("Nothing to do. No pages given. Missing -a option?")
    elif allpages:
        print("Dumping all pages…")
        Dumplr.BRUTEFORCELIMIT = bflimit
        Dumplr.FetchAllPages(targetpath,blog.rstrip('/'))
    else:
        Dumplr.FetchPages(targetpath,blog.rstrip('/'),pages)
